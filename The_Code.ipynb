{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for the algorithm\n",
    "\n",
    "Layer 1: Serves as the initial classifier to determine whether the individual's data corresponds to being \"Overweight\" or \"Not Overweight\".\n",
    "\n",
    "Layer 2: Acts as a secondary classifier, consulting Layer 1's output to guide further classification. If Layer 1 identifies the data as \"Overweight\", Layer 2 divides it into subcategories, distinguishing between \"Overweight\" and \"Obese\" and the different levels of both Overweight and Obese. Conversely, if Layer 1 determines the data is \"Not Overweight\", Layer 2 separates it into \"Normal weight\" and \"Insufficient weight\" categories.\n",
    "\n",
    "### Abreviations explained\n",
    "\n",
    "\n",
    "* Frequent consumption of high caloric food (FAVC)\n",
    "\n",
    "* Frequency of consumption of vegetables(FCVC)\n",
    "\n",
    "* Number of main meals (NCP)\n",
    "\n",
    "* Consumption of food between meals (CAEC)\n",
    "\n",
    "* Consumption of water daily (CH2O)\n",
    "\n",
    "* Calories consumption monitoring (SCC)\n",
    "\n",
    "* Physical activity frequency (FAF)\n",
    "\n",
    "* Time using technology devices (TUE)\n",
    "\n",
    "* Consumption of alcohol (CALC)\n",
    "\n",
    "* Transportation used (MTRANS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20758, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2     Insufficient_Weight\n",
       "1           Normal_Weight\n",
       "12         Obesity_Type_I\n",
       "6         Obesity_Type_II\n",
       "3        Obesity_Type_III\n",
       "8      Overweight_Level_I\n",
       "0     Overweight_Level_II\n",
       "Name: NObeyesdad, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "\n",
    "file=\"E:\\\\Programming\\\\Projects\\\\Python\\\\Obesity Comp\\\\train.csv\"\n",
    "\n",
    "df=pd.read_csv(file)\n",
    "\n",
    "#Check all possible outputs for NObeyes column\n",
    "print(df.shape)\n",
    "Types=df.iloc[:,17]\n",
    "\n",
    "no_dupes=(Types.drop_duplicates())\n",
    "no_dupes.sort_values(axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and how we work with their values\n",
    "\n",
    "* Gender: Male or Female(Binarify)\n",
    "\n",
    "* family_history_with_overweight: yes and no(Binarify)\n",
    "\n",
    "* FAVC: yes and no (Binarify)\n",
    "\n",
    "* SMOKE: yes and no(Binarify)\n",
    "\n",
    "* SCC: yes and no(Binarify)\n",
    "\n",
    "* CALC: 'Sometimes' 'Frequently' 'no' (Encode)\n",
    "\n",
    "* MTRANS: 'Public_Transportation' 'Automobile' 'Walking' 'Motorbike' 'Bike' (Encode)\n",
    "\n",
    "* CAEC:'Sometimes' 'Frequently' 'no' 'Always' (Encode)\n",
    "\n",
    "* Age,Height,Weight,FCVC,NCP,CH20, FAF,TUE: Variable numeric values(So no need to change but maybe use scaler later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for numerification because we cant use strings \n",
    "\n",
    "#A function that takes an input and checks if the input is in the list, default list is to check for non-overweight however later we can reuse this with our customized list\n",
    "\n",
    "def Binarification(x,list=['Insufficient_Weight','Normal_Weight']):\n",
    "    if x in list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#A lot of data in the dataset is labeled yes and no for those we will use the Binarification function \n",
    "#However, for data with sometimes, frequently,no, etc we will use encoder\n",
    "def encode_column(df, column_name):\n",
    "    unique_values = df[column_name].unique()\n",
    "    encoding_dict = {value: index for index, value in enumerate(unique_values)}\n",
    "    df[column_name] = df[column_name].map(encoding_dict)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets preprocess data with the functions above\n",
    "\n",
    "df['Gender'] = df['Gender'].apply(lambda x: Binarification(x, list=['Female']))\n",
    "df['family_history_with_overweight'] = df['family_history_with_overweight'].apply(lambda x: Binarification(x, list=['yes']))\n",
    "df['FAVC'] = df['FAVC'].apply(lambda x: Binarification(x, list=['yes']))\n",
    "df['SMOKE'] = df['SMOKE'].apply(lambda x: Binarification(x, list=['yes']))\n",
    "df['SCC'] = df['SCC'].apply(lambda x: Binarification(x, list=['yes']))\n",
    "\n",
    "encode_column(df, 'CALC')\n",
    "encode_column(df, 'MTRANS')\n",
    "encode_column(df, 'CAEC')\n",
    "#Layer 1 will be to train a model that will just tell the difference between Overweight and non-overweight data so we Binarify the 'NObeyesdad' column\n",
    "L1_data=df.copy()\n",
    "L1_data['NObeyesdad']=L1_data['NObeyesdad'].apply(Binarification)\n",
    "\n",
    "#encode_column(L1_data,'NObeyesdad')  #this was just a test ended up with 70% accuracy\n",
    "\n",
    "L1_X=L1_data.drop(columns='NObeyesdad')\n",
    "L1_Y=L1_data['NObeyesdad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(L1_X,L1_Y)\n",
    "def get_score(model, X_train, X_test, Y_train, Y_test):\n",
    "\t\tmodel.fit(X_train, Y_train)\n",
    "\t\treturn model.score(X_test, Y_test)\n",
    "\n",
    "sc_x= StandardScaler()\n",
    "\n",
    "x_train= sc_x.fit_transform(x_train)\n",
    "x_test=sc_x.fit_transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy: 0.9023121387283237\n",
      "\n",
      "LogisticRegression accuracy: 0.9678227360308285\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k=math.sqrt(len(y_test))\n",
    "\n",
    "#Testing different models\n",
    "\n",
    "#KNN\n",
    "classifier=KNeighborsClassifier(n_neighbors= int(k+1), p=2, metric='euclidean')\n",
    "score=get_score(classifier,x_train, x_test, y_train, y_test)\n",
    "print(\"KNN accuracy:\",score)\n",
    "#Logistic Regression\n",
    "classifier= LogisticRegression()\n",
    "score=get_score(classifier,x_train, x_test, y_train, y_test)\n",
    "print('\\nLogisticRegression accuracy:',score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the tests the Logistic Regression model scored the best while taking the least amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_X=sc_x.fit_transform(L1_X)\n",
    "L1_predictions=classifier.predict(L1_X)\n",
    "L1_model=classifier\n",
    "\n",
    "df['L1P']=L1_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Layer 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overweight = df[df['L1P'] == 0].reset_index(drop=False)\n",
    "Non_Overweight = df[df['L1P'] == 1].reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Overweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy: 0.8401459854014599\n",
      "\n",
      "LogisticRegression accuracy: 0.8656934306569343\n",
      "\n",
      "RandomForest accuracy: 0.9007299270072993\n"
     ]
    }
   ],
   "source": [
    "#Copying the Non overweight dataframe\n",
    "L2NO_data= Non_Overweight.copy()\n",
    "\n",
    "X=L2NO_data.iloc[:,2:18]\n",
    "Y=L2NO_data.iloc[:,18]\n",
    "\n",
    "\n",
    "Y.apply(lambda x:Binarification(x,list=['Normal']))\n",
    "\n",
    "X=sc_x.fit_transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(X,Y)\n",
    "#KNN\n",
    "classifier=KNeighborsClassifier(n_neighbors= int(k+1), p=2, metric='euclidean')\n",
    "score=get_score(classifier,x_train, x_test, y_train, y_test)\n",
    "print(\"KNN accuracy:\",score)\n",
    "#Logistic Regression\n",
    "classifier= LogisticRegression(max_iter=100000)\n",
    "score=get_score(classifier,x_train, x_test, y_train, y_test)\n",
    "print('\\nLogisticRegression accuracy:',score)\n",
    "\n",
    "classifier= RandomForestClassifier()\n",
    "score=get_score(classifier,x_train, x_test, y_train, y_test)\n",
    "print('\\nRandomForest accuracy:',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_NO_model=classifier\n",
    "prediction=L2_NO_model.predict(X)\n",
    "\n",
    "Non_Overweight['Prediction']=prediction\n",
    "\n",
    "#We can now change the binary values to non binary values so they can be appended later to the original df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we work with the overweight part of the dataset however since there are so many different outputs instead of binarification we will train it with dataset that is labeled for Overweight 1, Obese 2 and for damage control of the 3% of errors 3 for Normal or insufficient weight. They will then have numbers after them where 0 is level 1 and 1 is level 2,etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15281, 2)\n"
     ]
    }
   ],
   "source": [
    "def label(x, categories=['Normal_Weight', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']):\n",
    "    if x == categories[0]:\n",
    "        return 51\n",
    "    elif x == categories[1]:\n",
    "        return 80\n",
    "    elif x == categories[2]:\n",
    "        return 81\n",
    "    elif x == categories[3]:\n",
    "        return 82\n",
    "    elif x == categories[4]:\n",
    "        return 10\n",
    "    elif x == categories[5]:\n",
    "        return 11\n",
    "    else:\n",
    "        return 50\n",
    "    \n",
    "L2O_data= Overweight.copy()\n",
    "\n",
    "X=L2O_data.iloc[:,2:18]\n",
    "Y=L2O_data.iloc[:,18]\n",
    "X=sc_x.fit_transform(X)\n",
    "Y=Y.apply(label)\n",
    "\n",
    "def split_and_put_series(Y):\n",
    "    A = Y // 10\n",
    "    B = Y % 10\n",
    "    return pd.DataFrame({'A': A, 'B': B})\n",
    "\n",
    "Y_copy = split_and_put_series(Y)\n",
    "print(Y_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest: 0.885632033499084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "x_train, x_test, y_train, y_test= train_test_split(X,Y_copy)\n",
    "\n",
    "dt=RandomForestClassifier()\n",
    "Classifier= MultiOutputClassifier(dt)\n",
    "score=get_score(Classifier,x_train, x_test, y_train, y_test)\n",
    "print(\"RandomForest:\",score)\n",
    "\n",
    "\n",
    "L2_O_model=Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=L2_O_model.predict(X)\n",
    "temp=pd.DataFrame()\n",
    "temp['A']=predict[:,0]\n",
    "temp['B']=predict[:,1]\n",
    "\n",
    "\n",
    "\n",
    "def reverse_split_series(df):\n",
    "    Y = df['A'] * 10 + df['B']\n",
    "    return Y\n",
    "\n",
    "predict= reverse_split_series(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.07614155530610654\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "Overweight['Prediction']=predict\n",
    "\n",
    "def reverse_label(y, categories=['Normal_Weight', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III', 'Overweight_Level_I', 'Overweight_Level_II']):\n",
    "    reverse_mapping = {\n",
    "        51: categories[0],\n",
    "        80: categories[1],\n",
    "        81: categories[2],\n",
    "        82: categories[3],\n",
    "        10: categories[4],\n",
    "        11: categories[5],\n",
    "        50: 'Insufficient_Weight'  \n",
    "    }\n",
    "    return reverse_mapping.get(y)\n",
    "\n",
    "Overweight['Prediction']=Overweight['Prediction'].apply(reverse_label)\n",
    "\n",
    "combined_df = pd.concat([Overweight, Non_Overweight], ignore_index=True)\n",
    "# Sort the combined DataFrame by the specified index column\n",
    "combined_df.sort_values(by='id', inplace=True)\n",
    "\n",
    "df['prediction']=combined_df['Prediction']\n",
    "\n",
    "def cosine_similarity_score(column1, column2):\n",
    "    # One-hot encode the categorical columns\n",
    "    encoder = OneHotEncoder()\n",
    "    encoded_column1 = encoder.fit_transform(column1.values.reshape(-1, 1))\n",
    "    encoded_column2 = encoder.transform(column2.values.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = cosine_similarity(encoded_column1, encoded_column2)\n",
    "    \n",
    "    # Extract the similarity score from the matrix\n",
    "    similarity_score = similarity_matrix[0][0]\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "similarity = float(cosine_similarity_score(df['prediction'], df['NObeyesdad']))\n",
    "print(similarity)\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "def jaccard_similarity_score(column1, column2, average='macro'):\n",
    "    # Calculate Jaccard similarity\n",
    "    similarity_score = jaccard_score(column1, column2, average=average)\n",
    "    return similarity_score\n",
    "\n",
   
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"E:\\\\Programming\\\\Projects\\\\Python\\\\Obesity Comp\\\\test.csv\"\n",
    "\n",
    "df=pd.read_csv(file)\n",
    "\n",
    "#Now lets preprocess data with the functions above\n",
    "\n",
    "df['Gender'] = df['Gender'].apply(lambda x: Binarification(x, list=['Female']))\n",
    "df['family_history_with_overweight'] = df['family_history_with_overweight'].apply(lambda x: Binarification(x, list=['yes']))\n",
    "df['FAVC'] = df['FAVC'].apply(lambda x: Binarification(x, list=['yes']))\n",
    "df['SMOKE'] = df['SMOKE'].apply(lambda x: Binarification(x, list=['yes']))\n",
    "df['SCC'] = df['SCC'].apply(lambda x: Binarification(x, list=['yes']))\n",
    "\n",
    "encode_column(df, 'CALC')\n",
    "encode_column(df, 'MTRANS')\n",
    "encode_column(df, 'CAEC')\n",
    "#Layer 1 will be to train a model that will just tell the difference between Overweight and non-overweight data so we Binarify the 'NObeyesdad' column\n",
    "data=df.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "X=sc_x.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predict=L2_O_model.predict(X)\n",
    "temp=pd.DataFrame()\n",
    "temp['A']=predict[:,0]\n",
    "temp['B']=predict[:,1]\n",
    "\n",
    "\n",
    "\n",
    "predict=pd.DataFrame()\n",
    "\n",
    "predict['Prediction']= reverse_split_series(temp).apply(reverse_label)\n",
    "\n",
    "\n",
    "id=df['id']\n",
    "\n",
    "Answer=pd.DataFrame()\n",
    "Answer['id']=id\n",
    "Answer['Prediction']=predict\n",
    "\n",
    "Answer.to_csv('Submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
